\chapter{Method}
\label{chap:method}

This chapter entails the details of the neural network such as the architecture, loss/objective function along with the training and evaluation procedures of the proposed method.

\section{Architecture}
To predict root-relative 3D pose solely using 2D poses, a hybrid network using \ac{vae} and \ac{gan} as discussed in \ref{subsec:vaeganhybrid} is employed. In contrast to the \ac{vae}-\ac{gan} hybrid proposed by \cite{autoencoding_beyond_pixels}, the \ac{gan} loss gradient is propogated to both the decoder and the encoder. In other words, the \ac{vae} as a whole is considered as the generator network. The overall architecture is conposed of 3 model Encoder, Decoder and Discriminator as illustrated in the Fig \ref{fig:method_arch}. This section eloborates on each of this model and how they are interconnected. The motivation regarding other components tha make up the neural network such as dropout, activation and optimization functions are discussed later in chapter \ref{chap:experiments}, Experiments.

\subsection{Encoder}
Adding to the explanation of \ac{vae} in \ref{subsec:vae}, the role of the encoder $Q$, is to take a 2D pose $\textbf{x}_i = (x_i, y_i)$ with its root located at the joint and of upper body length of $c$ units as an input. Where $i = 1 ... J$ and $J$ denote the number of joints of the pose. And output the corresponding embedding in a $d$ dimensional latent space in the form of mean $\mu$ and standard deviation $\sigma$ of each dimension.

\begin{equation} \label{eqn:Q_fn}
    \begin{gathered}[b]
        Q_{\theta_q}(\textbf{x}) = \mu, \sigma
    \end{gathered}
\end{equation}

Where $\theta_q$ denotes the learned parameters of the encoder during training. This encoder is composed of an upsampling layer that scales the $2\!\cdot\!J$ dimensional input to match the number of hidden neurons $h$ of the encoding module. The encoding module $q$ is made of $n$ residual block composed of 2 \ac{fc} layers following the related works, to allow comparison. The encoding block is followed by 2 \ac{fc} (linear) layers that downsample the hidden representation of dimension $h$ to match the latent space dimension $d$. The output of the two downsampling layers represents the mean and standard deviation respectively. However in practice, the encoder is designed to predict log-variance instead of the standard deviation to have a better distribution of values and gradient. The layers of the encoder, decoder and the discriminator models are kept similar for simplicity and as done by the related works. The residual blocks used in all the 3 models are identical and is illustrated in the Fig \ref{fig:residual_block}.

\begin{figure}[h] 
    \centering
    \includegraphics[width=0.45\textwidth]{figures/arch/res_block.png}
    \caption{Illustration of the residual block made of 2 Linear layers of the same input and output dimensionality. These blocks are repeated $n$ times based on the configuration of the model.
    }
    \label{fig:residual_block}
\end{figure}

\subsection{Decoder}
The decoder $P$ takes the 2D pose embedding $z$ derived from the mean and log-variance predicted by the encoder and estimates the corresponding 3D pose $\textbf{Y}_i = (x^\prime_i, y^\prime_i, z^\prime_i)$. Similar to rhe encoder, $i = 1 ... J$ and $J$ denote the number of joints of the pose. The reparametrization trick is used to make the process differentiable and induce variance. This is done by scaling the standard deviation obtained from the log-variance with a random sample $\epsilon$ from a unit gaussian distribution $\mathcal{N}(0,1)$. The sum of the scaled standard deviation and the mean gives the sample $z$.

\begin{equation} \label{eqn:P_fn}
    \begin{gathered}[b]
        z = \mu + \sigma \odot \epsilon \\
        P_{\theta_p}(z) = \textbf{Y}
    \end{gathered}
\end{equation}

Where $\theta_p$ represents the learned parameters of the decoding network. Similar to the encoder, the decoder consists of an upsampling layer that scales the sample $z$ of dimensionality $d$ to match the number of hidden neurons $d$ in the decoding module. The decoding module is identical to the encoding module and consists of $n$ residual blocks composed of 2 \ac{fc} layers. This decoding module is followed by a \ac{fc} layer to downsample the neurons to predict 3D pose of dimensions $3\!\cdot\!J$. As the goal is to predict root relative pose with root at origin, the output joints are considered to be relative to the origin (0, 0, 0).

Since the predicted 3D pose is to be projected to back to 2D pose to form the reconstruction loss, the distance between the head and the pelvis joint should approximately be of unit length as explained in \ref{sec:processing}. To achieve this, a Tanh activation function is used at the downsampling/output layer to obtain the predicted 3D pose (all joints) in the range [-1, 1]. %TODO

\subsection{Reprojection}
\label{subsec:reproject}
Referring to the camera modeling section \ref{sec:camera_projection}, the 3D that corresponds to the pre-processed 2D pose is located at a fixed distance $c$ from the camera. The 3D pose is considered to be predicted relative to the origin for simplicity and symetry and is to be translated $c$ units aways from the camera. Another required adjustment is the scale. The range of coordinates predicted is in the range [-1,1]. But the length of the lower half of the pose can be longer that the upper half and usually is the case. Hence the predicted 3D pose is scaled by a factor of 1.3, which is the ratio of the mean length of the upper and lower halves. This is to enforce that the length of the upper half is ~1 unit, while covering the true range of the lower half and get the best 2D re-projection.

This scaled 3D pose prediction is directly projected from the same point of view to get a 2D projection that corresponds to the input 2D pose. This similarity is used as the reconstruction loss for a contrained optimization of the \ac{vae}.

\begin{equation} \label{eqn:proj_direct}
    \begin{gathered}[b]
        \textbf{Y}^\prime = \textbf{Y}*1.3 + (0,0,c) \\
        \textbf{y} = PP(\textbf{Y}^\prime)
    \end{gathered}
\end{equation}

Where PP refers to perspective projection and \textbf{y} denotes the 2D projection that correspond to the input 2D pose. The scaled 3D pose is also randomly rotated by uniformly sampling an azimuth angle from the range [$-\pi, \pi$] and elevation range in the range [$-\pi/9, \pi/9$]. The 2D pose obtained from the projection of this rotated 3D pose gives a different point of view, a \textbf{\textit{novel view}} of the 3D pose that is different from the views of the subject in the dataset. This novel view is used for unconstrained optimization of the \ac{vae} using the discriminator. The elevation angle has been followed by other works but is not observed to have any visible impact.

\begin{equation} \label{eqn:proj_rotated}
    \begin{gathered}[b]
        \textbf{Y}^\prime_{rot} = \textbf{R} * (\textbf{Y}*1.3) + (0,0,c) \\
        \tilde{\textbf{y}} = PP(\textbf{Y}^\prime_{rot})
    \end{gathered}
\end{equation}

Where \textbf{R} referes to the rotation matrix formed using the randomly sampled angles. $\tilde{\textbf{y}}$ refers to novel view of the 3D pose.

\subsection{Discriminator}
The discriminator takes the novel view 2D pose of the predicted 3D pose $\tilde{\textbf{y}}$ along with the real 2D poses \textbf{x} from the dataset as the input and classifies which 2D pose belongs to which cateogory, real or fake (generated novel view). The real and novel views need not correspond to the same pose, the goal of the discriminator is to learn the general ability to distinguish the 2D poses from the dataset (real) from the predicted poses (fake).

\begin{equation} \label{eqn:proj_rotated}
    \begin{gathered}[b]
        D_{\theta_d}(\tilde{\textbf{y}} \cup \textbf{x}) = \left\lbrace p_{class}^i | i \in {1, .. n(\tilde{\textbf{y}} \cup \textbf{x})}\right\rbrace \\
        where, 0\leq p_{class} \leq 1
    \end{gathered}
\end{equation}

Where $\theta_d$ refers to the learned parameters of the discriminator and $p_{class}$ here denotes the probabilty of the pose's class, with 1 being real and 0 being fake. The discriminator mimicing the encoder and the decoder, upsamples the $2\!\cdot\!J$ dimensional novel 2D pose to $h$ dimensions of the main module. The main module just as other models is composed of $n$ residual blocks. It is important to note that $n$ need not be the same for all the 3 models. The learned features are downsampled to predict the probabilty of the pose being real or fake. The sigmoid activation function is used at this last layer as the required values are in the range [0, 1]. %TODO

\begin{figure}[h] 
    \centering
    \includegraphics[width=\textwidth]{figures/arch/method_arch.png}
    \caption{Illustration of the neural architecture of the proposed method. The network components in blue, encode the 2D pose to a latent representation in terms of mean $\mu$ and standard deviation $\sigma$ of the distribution. While the components in pink, sample $z$ from this latent space and decode the corresponding 3D pose. The 3D pose is projected directly to 2D space for a constrained optimization using input 2D pose in the camera view and randomly rotated and projected for unconstrained optimization using the discriminator $D$ in a novel angle. The data that contributes to the loss function in orange, are mapped with a dotted line.
    }
    \label{fig:method_arch}
\end{figure}

\section{Loss Functions}
\label{sec:loss_fn}
The loss functions used in this \ac{vae}-\ac{gan} are similar to the ones explained in \ref{subsec:vaeganhybrid}. The hybrid training proposed in \cite{autoencoding_beyond_pixels} uses the similarity between the prediction and input estimated by the discriminator \textit{replacing} the element-wise loss. While the proposed approach exploits the element-wise loss and the similarity loss by using them on different transformations of the predicition (direct view 2D and novel view 2D). The formulation of each loss function of the hybrid with respect to the human pose data is as follows.

\subsection{Reconstruction Loss}
Since the goal of the proposed method is to learn 3D pose without requiring any of such training data, the approach uses 2D pose as its own supervisory data. This self-supervision is achieved by reprojecting the 3D pose in the same view to retireve the 2D pose that should ideally be the same as the input 2D pose as discussed in the Reprojection \ref{subsec:reproject} part of the previous section. The objective of the model here is to converge the distance (element-wise) between the predicted 2D pose $\textbf{y}$ and the input 2D pose $\textbf{x}$. L1 loss is used to measure the element-wise reconstruction error.

\begin{equation} \label{eqn:loss_recon}
    \begin{split}
        \mathcal{L}_{\text {recon}}  & = L1\_loss(\textbf{x}, \textbf{y}) \\
        & = \sum_{i=1}^J |\textbf{x}_i - \textbf{y}_i|
    \end{split}
\end{equation}

The training the hybrid network with 3 different losses is not very stable. For the reconstruction loss, there is absolute target and metric for learning. So this is a contrained optimization and directly corresponds to the error of the 3D pose. Hence it is desired to give give high priority and keep this loss relatively stable and low. All the 3 loss terms are weighted to be summed to one final object term that is used to update the network. The weight coeffiecient $\lambda_{recon}$ is kept constant and relatively higher than the other two that are introduced next.

\subsection{Latent Prior Loss}

The proposed hybrid though named \ac{vae}-\ac{gan}, actually uses a \ac{bvae}, it is not highlighted as the only differnece is having a varible coeffcient rather than fixing it to 1. Refering to the explanation of the \ac{bvae} in \ref{subsec:bvae}, the higher the weight of the \ac{kld} term the better is the disentaglement or clustering of the mebeddings are in the latent space. There is no magic $\beta$ that gives the best recosntruction and representation. This is a trade-off and the value is chosen based on the requirements. Though both are the priorities of this work, more importance is given to the reconstruction as it is quantitative. It is important to note that the weight coefficient $\lambda_{KLD}$ to balance the loss terms is separated for the $\beta$ term to have better control when experiment with different scheduling stategies for the $\beta$ term. Cyclic beta techique similar to \cite{cyclicbeta} is used to handle the vanishing \ac{kld} problem, where $\beta$ is increased from 0 to true value every $b$ epochs and observed to improve training.

\begin{equation} \label{eqn:loss_kld}
    \begin{split}
        \mathcal{L}_{\text {prior}} = \mathcal{L}_{\text {KLD}} &= -0.5 * \sum (1 + log(\sigma^2) - \mu^2 - \sigma^2) \\
        &= -0.5 * \sum (1 + logvar - \mu^2 - e^{logvar})
    \end{split}
\end{equation}

The $logvar$ term here is the output of one of the downsampling layers of the encoder in practice. The terms denoted in the equation are per sample and in practice the losses are averaged and normalized based on the dimensionality. The normalization is included in their $\lambda$ coeffcients.

\subsection{Discriminator Loss}

The discriminator takes both the samples from the real dataset \textbf{x} and predicted novel 2D poses $\tilde{\textbf{y}}$ and classifies the samples. For making such training data the values of 1s and 0s are given as the labels for the real and generated 2D poses respectively. As the labels are self generatated the discriminator training is self-supervised as well. In constrast to the reconstruction error, the error in discriminator's classification is not expected to be minimized even though the models are learning to minimize this mertic. Rather, the role it plays is to help the generator produce 3D poses that give more realistic 2D novel views. That is, 2D projections from random views of the 3D pose that are indistinguishable from the 2D pose from the real dataset. Fig \ref{fig:novel_view_constraint} illustrates the differences in the novel views of realistic and unrealistic 3D poses that this adverserial training aims to resolve. Hence this error is used to optimize both the discriminator and generator models that actually produce the 3D poses.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/h36_viz/novel_view_contraint.pdf}
    \caption{Illustration of the differences in accurate or realistic 3D pose predicitions from inaccurate predicitions. Modified image from \cite{can3dpose}}
    \label{fig:novel_view_constraint}
\end{figure}

The real and the fake 2D poses need not be related in anyway. Infact, in practice the loss terms of the real and fake data is computed and used to update the models separatly, as suggested in \cite{soumith2017wasserstein, goodfellow2014generative}. As the goal of the discriminator is to just predict the correct label of the 2D pose, the \ac{bce} loss function is used. 

\begin{equation} \label{eqn:loss_bce}
    \begin{split}
        \mathcal{L}_{\text {Disc.}} & = BCE\_loss(y, \hat{y}) \\
        & = -1 * (\hat{y} \cdot \log y + (1 - \hat{y}) \cdot \log (1 - y)) \\
        & Where, \:\: y \in D_{\theta_d}(\tilde{\textbf{y}} \cup \textbf{x}) \;\; and \;\; \hat{y} \in (0s, 1s)
    \end{split}
\end{equation}

The $\tilde{\textbf{y}}$ here is the transformed output of the decoder. That means, to minimize this loss the decoder model is to be updated along with the discriminator. However as discussed earlier we consider the encoder to be part of the generator. Hence all the models use this metric to update their weights. Again inconstrast to the reconstruction loss, the discriminator loss does not give direct signal to correct the 3D pose and thus updating the models to minimize this loss is an unconstrained optimization and is not stable. Hence the weight coefficent $\lambda_{disc.}$ is not as high as that of the reconstruction loss. As this is a min-max game where the generator tries to fool the discriminator while the discriminator tries to guess which same is from the generator, all the models cannot be updated at the same time. The training procedure to enable the models to learn and improve the generated 3D pose is discussed in the next section.

%TODO more in practice here?

\section{Training}

\subsection{Procedure}
The general training procedure is summarized before describing specific details. In this min-max game the generator produces samples and the discriminator verifies them, the generator uses this verfication and in our method it also uses self-supervision loss to improve itself in both aspects. The standard practice in trianing \acp{gan} is to start with generating the samples using the generator. As the discriminator's evalutation is required to teach the generator, the discriminator is first trained to distinguish reals from fakes given by the generator. The samples are again given to the trained discriminator to get the loss to train the generator. That is, the generator is kept constant while updating the discriminator and the discriminator is kept constant while updating the generator. This turn-wise play is important without which the models would be trying to hit a moving target as put by \cite{gan_training_google}.

While the procedure seems straightforward, \ac{gan} training is finding a Nash equilibribrium to a two-plaer non-coorperative game. There exists no feasible algorithm to find the equilibrium for a problem of such complexity\cite{improved_gan}. As the training initiates, the genertor produces 3D poses close to noise and thus is very easy for the discriminator to learn to classify. This overpowers the discriminator as it predicts fakes with very high confidence and the loss tends to 0. The generator can not learn properly if the discriminator does not provide proper signal. This happens when the discriminator does not perform well or if it performs too well where the loss is close to 0 and not helpful \cite{gan_wgan_tutorial}. Other common challenges of \ac{gan} training and  is explained in \ref{subsec:gan}. The widely practiced tricks to improve the training of \acp{gan} are discussed later in \ref{chap:experiments}.

%FIXME few details missing - more about problems of gans? how implemented? idk idc


%TODO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\subsection{Activation and Optimization}
beta scheduling cycling and o to 1 annealing, weighting the loss components 

\section{Evaluation Metrics} % FIXME
3D \ac{hpe} and Human3.6M in particular is mainly evaluated by \ac{mpjpe} metric. \ac{mpjpe} as it abbreviates is the mean of the position estimate for all the joints of a pose. Where per-joint position estimate is nothing but the euclidian distance (usually measured in mm) between the predicted joint to its ground truth.


%FIXME dropout
% Following the best practices from \cite{soumith2017wasserstein}, the decoder (and the encoder) which acts as the generator for the \ac{gan} is made to output 3D poses with values from [-1,1]. 
% %FIXME blunder in the implementation?
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% initalization activation optimier
% at activation mention that the scaling of -1 again?
%TODO
% add parameters of the netowrks number of layers hidden dimension root at 0 16 joints prediciton 17 joints


% \section{more details the model prints?} % FIXME
%TODO add all models in apendix

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/h36_viz/human_pose_x0001.png}
%     \caption{Illustration of the differences in accurate or realistic 3D pose predicitions from inaccurate predicitions. Modified image from \cite{can3dpose}}
%     \label{fig:x}
% \end{figure}
