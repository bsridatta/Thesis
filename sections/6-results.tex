\chapter{Experiments}
\label{chap:experiments}

%TODO add wgan experiments
\section{Supervised Baseline}
To the best of the author's knowledge, the closest work using a \ac{vae} is a cross-modal training approach in 3D hand pose estimation \cite{crossmodal}. As there is no similar approach in \ac{hpe} using a \ac{vae}, a supervised model using just a \ac{vae} and 3D ground truth pose data is implemented. This is to verify the feasibility of a \ac{vae} and find a set of hyperparameters that worked well for the H3.6M dataset under the supervised setting. This served as a baseline over which an unsupervised \ac{gan} was built upon. Hyperparameters such as the number of neurons in the layers, the number of residual blocks $n$, dimensions of the latent space, etc are taking as reference.

\section{Unsupervised GAN}
Building upon the supervised \ac{vae}, the proposed architecture is built. The following choice is kept standard for most of the experiments and could be used to reproduce the results. All the layers are made of 1024 neurons. The encoder and the decoder are made of 1 residual block each ($n = 1$) while the discriminator is made of 2. This is to keep the capacity of the generator and the discriminator similar. Various forms of scaling the 2D poses are experimented with to make the cycle of lifting and projection smooth. The scaling mentioned in the preprocessing section where the distance between the root joint and head worked the best. %TODO is this need? is it enough? 

The results presented in the following sections are after training the networks for $\sim$400 epochs (~5.5 hours) on approximately 300,000 2D poses with a batch size of 2560 on an Nvidia Titan X. The input poses are flipped with a probability of 0.5. The model takes 16 joints as the output where the root is added at the origin for validation. The proposed architecture consists of 1024 hidden units per linear layer and 51 latent dimensions. Both the \ac{vae} and the discriminator are trained using Adam optimizer with default hyperparameters and with a learning rate of 2e-4. The gradient norms of the discriminator are clipped to 1 when training the discriminator. While training the generator the gradient norms are clipped to 2 for all the models while the gradient values are clipped to 1000.

One of the challenging parts is finding the optimal weights for each of the terms in the triplet loss. The loss coefficients $\lambda_{recon}$, $\lambda_{\acs{kld}}$, $\lambda_{disc.}$ are set to 1, 0.001, 0.001 respectively. The higher weight is motivated by 2 reasons. $\lambda_{recon}$ refers to the constrained optimization and irrespective of how realistic it is, projection loss is desired to be consistently low to get better \ac{mpjpe}. That leads to the other reason that the quantitative results are given higher importance.
%TODO add loss standardization values to kld

The values of $\lambda{\acs{kld}}$ and $\lambda{disc.}$ can be tuned according to the task at hand based on how well the poses are to be clustered or how important it is to reject poses that are not realistic. The $\beta$ value for the \ac{vae} is cycled from 0 to $\lambda_{\acs{kld}}$ every 40 epochs. While keeping it constant at $\lambda_{\acs{kld}}$ for 10 epochs with a 10 epoch warmup at the beginning of the training.

\section{Quantitative Results}

The results obtained by the networks with the above configuration in addition to the choices mentioned in \ref{chap:method} are summarized in Table \ref{table:result_zv}. The summaries of the models are provided in appendix \ref{chap:summaries} to help reproduction.

\input{tables/result_zv.tex}

\textbf{SH} refers to the results using 2D Stacked Hourglass detections as input. These detections are noisy and directly affects the predictions of the model. And \textbf{DA} refers to using Domain Adaptation network to include additional datasets and \textbf{TD} denotes the use of temporal data. It is important to note that the proposed unsupervised method and the one presented in \cite{amazon1} are the only methods that do not predict the scale of the 3D pose due to the processing technique.

\input{tables/zv_mean_a_j}

The best hypothesis, \textbf{BH} has improved the results by ~2.7 mm which is considerable in comparison to the equivalent gain in \cite{amazon1} using domain adaptation network with more data and temporal information. Since there is no technique to pick the best hypothesis without having access to the ground truth, the results referred to, are the ones obtained using \textbf{ZV} unless specified otherwise. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/results/violin_pjpe.pdf}
    \caption{Visualization of the MPJPE error distribution under Protocol $\#2$ for each action.}
    \label{fig:mpjpe_trends}
\end{figure}

The average \ac{mpjpe} errors for each action is presented in Table \ref{table:pjpe_a}, and Fig \ref{fig:mpjpe_trends} illustrates the distribution of these errors. The \ac{mpjpe} error per joint increases as we move from inner to outer joints. This can be observed in Table \ref{table:pjpe} which shows the average per joint position error. The error for the Pelvis joint is not mentioned as it is assumed to be at the origin and is always zero. The larger errors in the limb joints are due to the higher variation in their location throughout the data compared to joints like the neck or the nose. Moreover, these joints are often occluded by the person's body especially when doing actions such as sitting. The 2D projections of a pose from sitting posture is clustered to a smaller region from certain \acp{pov}. This makes it challenging to estimate the scale and depth of each joint and leads to outliers as visualized in the violin plots.

Since the data consists of an equal mix of data from different \acp{pov}, the predictions from the \ac{pov}, where the limbs are in the \ac{fov} are more informative than the other \acp{pov}. When the limbs are not in the \ac{fov}, the model learns to guess their location based on other features such as articulation or posture of the body. The effect of \ac{pov} is the possible cause of the bimodal error distribution in actions such as purchases, eating, phoning. Since the actions are hands specific, they are restricted to a particular region making the pose more predictable from one \ac{pov} and less from another. These challenges are inherent to the task of pose lifting and in many cases are challenging even for a human eye.


\section{Qualitative Results}
\subsection{Average Cases}
Since pose lifting is a tough task for humans as well, the errors around and below 50mm are usually not identifiable without 3D ground truth reference. To give a better perspective and sense of depth, 3D models are used to visualize the model predictions. Instead of handpicking predictions, poses that have errors closest to the mean of the respective action are selected and visualized in Fig \ref{fig:zv_mean}. The maximum mean \ac{mpjpe} per action as presented in table \ref{table:pjpe_a}, is not more than 75 mm and thus the predictions (in blue) are not very different from the ground truth pose (in pink). The limb joints especially the elbows and the wrist are occluded by the body in the majority of the visualized poses. Consider the 2D poses where the subject is seen in a side view, ignoring the RGB background it is quite ambiguous to say which limb is forward and which is back. The network has guessed the location of these joints to a good extent. The postures in the actions such as sitting down, sitting on a chair, etc have been predicted well.

\input{tables/zv_mean}
\input{tables/zv_fail}

\subsection{Failure Cases}

Similarly, the poses with the maximum errors (outliers) are taken for each action and visualized in Fig \ref{fig:zv_fail}. The main reason as mentioned earlier is the failure in estimating the depth or in predicting a good representation of the 2D pose for the decoder. One of the best examples of ambiguity can be seen in the visualizations where the 3D pose is very plausible, with very good 2D reprojection but with the subject facing the wrong direction. It can be observed from some of the visualizations in \ref{fig:zv_fail}, that the predicted joints are consistent with respect to each other but are away from the root joint. Note that the root joint is always added to the prediction at the origin. Since the rest of the pose is consistent it could be implied that the model failed to predict the depth of the joints properly. Consider the model predicts a 3D pose whose 2D projection is deviated from the input 2D. This error is magnified in 3D if the joint is farther away from the image. To overcome such errors, higher weight is given to the 2D reconstruction loss.

%FIXME add an example for all actions


\section{Latent Space}
A good representation of the 2D poses in the latent space not only helps the decoder to construct better 3D poses but also shows the models understanding of the pose in terms of the articulation/posture, and the \ac{pov}. Though the ZV model used to report these results is not tuned to have the best clustering, it still should learn an efficient representation to encode the 2D poses in the latent space. In order to evaluate the encoder capabilities, the test dataset consisting of $\approx 8000$ 2D poses are encoded to $51$-dimensional latent space. These embeddings $z$ are further reduced to two dimensions using \ac{umap} \cite{umap}. The clustering is computed using $50$ neighbors and $0.1$ units of minimum Euclidean distance and the reduced embeddings are visualized in Fig \ref{fig:latentspace}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/results/umap.pdf}
    \caption{Visualization of the 2D pose embeddings in the latent space using UMAP dimensionality reduction.}
    \label{fig:latentspace}
\end{figure}

The reduced $z$ are colored with respect to their action. This is not an ideal way to examine the clustering as there is significant overlap in the actions during their 'transition phase'. For example, as the data is collected as the subject performs that action, the subject would be 'standing' before 'sitting down'. Since it is not practical to label each frame, the action labels are used. Despite the overlap, patterns and clusters can be seen indicating a good representation capability of the encoder. To further evaluate the extent of clustering, 5 nearest latent neighbors of the embeddings of a few interesting poses are queried to verify if they represent common distribution. The corresponding 2D poses of these embeddings are visualized in Fig \ref{fig:neighbors}. Strong similarity can be observed among the nearest neighbors. Similarities include a set of joints having the same articulation, natural transition from one posture to another, or a different \ac{pov} of the similar pose. 

\input{tables/neighbors.tex}
%TODO add a downstream task for embedding video sync multi-view


\section{Missing Joints}
The other important problem the proposed method addresses is the ability to handle missing points. The intuition is from the ability of the encoder to represent similar poses together in the latent space as seen in \ref{fig:neighbors}. A 2D pose missing a joint or two should still be able to tell about the posture and the view of the subject. The encoding of such incomplete poses should be closer to the means of similar pose distributions. Given the decoder can produce plausible 3D poses, lifting the incomplete 3D pose should be at least plausible if not similar to the ground truth 3D pose. Hence the network should be able to 'hallucinate' the missing joints. To evaluate this, the model is given incomplete 2D poses obtained by randomly zeroing 1 and 2 joints. The models are not trained explicitly on any incomplete poses in order to verify the above hypothesis. The \ac{mpjpe} using \ac{zv} and \ac{bh} are reported in table \ref{table:missing_joints}. As expected, there is a significant difference between \ac{zv} and \ac{bh} as there would be many possible locations for the missing joints among which the ground truth pose would be one.

\begin{table}[htb!]
    \centering
    \begin{tabularx}{\linewidth}{XXX}%{llcc}
        \toprule
        \# Missing Joints & Zero Variance (ZV) & Best Hypothesis (BH)\\
        \midrule \midrule
        0                 & 52.74         & 50.3            \\
        1                 & 79            & 53              \\
        2                 & 101           & 65              \\
        \bottomrule
    \end{tabularx}
    \caption{Results on Human3.6M in MPJPE (in mm) under Protocol $\#2$ using ground truth 2D pose with \textbf{missing joints} as input.}
    \label{table:missing_joints}
    \vspace{-3ex}
\end{table}